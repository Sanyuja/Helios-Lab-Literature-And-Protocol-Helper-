
## ğŸŒŸ Project Overview

**Helios-Lab** is a Retrieval-Augmented Generation (RAG) assistant that helps researchers and analysts quickly navigate **scientific literature** and **lab protocols**.

Instead of manually skimming dozens of PDFs and protocol documents, a user can ask natural-language questions like:

> â€œWhat controls were used for the calcium imaging assay last year?â€
> â€œHow did we handle overnight incubation for the XYZ antibody?â€
> â€œWhat sample sizes were typical for our behavioral experiments?â€

The system:

* **Indexes internal protocols, lab notebooks, and reports**
* **Indexes external scientific papers and references**
* Uses **FAISS** for vector search over embedded chunks
* Uses **OpenAI LLMs** to generate grounded answers
* Enforces **prompt guardrails and traceability** so every answer is backed by specific documents, page numbers, and DOIs or protocol IDs

Internal testing showed that **Helios-Lab reduced literature and protocol review time by about 60%** for common recurring questions.

---

## ğŸ¯ Key Goals

1. **Speed up** the process of answering protocol and methods questions
2. **Reduce duplication** of effort across researchers
3. **Increase traceability**: every answer must show *where it came from*
4. **Minimize hallucinations** through guardrails and strict prompting
5. Provide a **reusable, domain-agnostic RAG template** that could be adapted later to fraud documentation, policy search, or risk playbooks

---

## ğŸ§± High-Level Architecture

```text
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Document Ingestion           â”‚
          â”‚  (PDFs, DOCX, Markdown)       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Parsing & Chunking           â”‚
          â”‚  (text extraction, sections)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Embedding Generation         â”‚
          â”‚  (OpenAI embeddings)          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Vector Index (FAISS)         â”‚
          â”‚  + Metadata Store             â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  RAG Query Pipeline           â”‚
          â”‚  Retrieve â†’ Compose Prompt    â”‚
          â”‚  â†’ Generate Answer            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Guardrails & Traceability    â”‚
          â”‚  Citations, chunk IDs, links  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

* **Language**: Python 3.10
* **LLM**: OpenAI GPT-4 class model (via API)
* **Embeddings**: OpenAI embeddings (`text-embedding-3-large`)
* **Vector Index**: FAISS (flat index with inner product similarity)
* **Metadata Store**: SQLite or PostgreSQL for doc and chunk metadata
* **Orchestration**: Simple scheduled jobs via cron or n8n / Airflow
* **Service layer**: FastAPI for HTTP API; optional Streamlit UI for internal users
* **Storage**:

  * Raw documents in S3
  * Parsed text + metadata in S3 or database
  * FAISS index persisted to disk / object store

---

## ğŸ“‚ Directory Structure

```text
helios-lab/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml              # paths, model names, thresholds
â”‚   â””â”€â”€ logging.yaml             # logging setup
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ fetch_documents.py       # load new PDFs/DOCX from S3 or drive
â”‚   â”œâ”€â”€ parse_pdf.py             # PDF â†’ text with sections & page numbers
â”‚   â”œâ”€â”€ parse_docx.py
â”‚   â””â”€â”€ normalize_metadata.py    # title, authors, protocol_id, doi, etc.
â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ chunk_text.py            # section-based + token-based chunking
â”‚   â””â”€â”€ chunk_strategies.py      # different chunk rules (methods-heavy, etc.)
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ generate_embeddings.py   # uses OpenAI embedding API
â”‚   â””â”€â”€ index_faiss.py           # build/update FAISS index
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ retrieve.py              # retrieve top-k chunks from FAISS
â”‚   â”œâ”€â”€ prompt_builder.py        # builds LLM prompt with instructions + context
â”‚   â”œâ”€â”€ guardrails.py            # enforce citation, style, refusal logic
â”‚   â””â”€â”€ answer_generator.py      # call OpenAI and post-process results
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ server.py                # FastAPI app (ask-question endpoint)
â”‚   â””â”€â”€ schemas.py               # request/response models
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ build_eval_set.py        # generate benchmark questions
â”‚   â”œâ”€â”€ run_eval.py              # measure faithfulness, relevance, latency
â”‚   â””â”€â”€ human_review_template.md # template for manual annotation
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py                   # optional Streamlit front-end
â””â”€â”€ README.md
```

---

## ğŸ”„ Data & Document Flow

### 1ï¸âƒ£ Document Ingestion

Supported sources:

* Internal lab protocols (PDF or DOCX)
* Internal experiment reports (PDF or markdown)
* Selected external papers (e.g., downloaded PDFs with DOIs)
* Internal wiki exports (markdown)

Steps:

1. List files in S3 prefix or local directory (e.g., `s3://helios/raw/protocols/`).
2. For each new document:

   * Assign a unique `document_id`.
   * Extract metadata:

     * `title`, `authors`, `year`
     * `protocol_id` (for internal docs)
     * `doi` or URL (for external papers)
     * `document_type` (protocol, paper, report)
3. Store metadata in a metadata database table: `documents`.

---

### 2ï¸âƒ£ Parsing & Chunking

#### Parsing

* Use PDF parser that preserves **page numbers** and **section headers** (`Introduction`, `Methods`, `Results`, `Discussion`, `Materials and Methods`, etc.).
* For DOCX, use `python-docx` and infer headings from styles.

Each document is split into logical sections:

* Title and abstract
* Methods / Protocol
* Results
* Notes / Appendices

#### Chunking Strategy

We use a **hybrid chunking** strategy:

* Prefer to break text by **section and subsection**, then
* Within each section, split into chunks targeting **500â€“1000 tokens** max
* Each chunk stores:

  * `document_id`
  * `chunk_id`
  * `section_name`
  * `page_start`, `page_end`
  * `text`

Chunks are stored in a `chunks` table and as JSONL in storage.

---

### 3ï¸âƒ£ Embedding & Indexing

* For each chunk, call OpenAI embeddings with `text-embedding-3-large`.
* Save embeddings as vectors (in a NumPy array) plus a mapping from vector index â†’ `chunk_id`.

FAISS index:

* Use an index type like `IndexFlatIP` (inner product).
* Add all embeddings.
* Save:

  * `faiss.index` file
  * `index_metadata.json` (mapping vector position â†’ `chunk_id`)

Versioning:

* Index versions are tagged by date and corpus (e.g., `v2025-01-10_protocols`).
* Older index snapshots remain available for reproducibility.

---

### 4ï¸âƒ£ RAG Query Pipeline

When a user sends a question:

1. **Query preprocessing**

   * Normalize whitespace
   * Optionally detect intent (e.g., method, control, sample size, reagent)

2. **Embedding & retrieval**

   * Embed the user query using the same embedding model.
   * Search FAISS for top `k` relevant chunks (e.g., `k = 10`).
   * Filter or rerank chunks:

     * Prioritize methods sections when asking â€œhowâ€
     * Prioritize results sections when asking â€œwhat happenedâ€
     * Filter out chunks below a similarity threshold

3. **Prompt construction**

   * Build a system prompt that includes:

     * Role: â€œYou are an assistant that only answers using the provided scientific context.â€
     * Rules:

       * Must **cite document_id and page number**.
       * Must not invent protocols or results.
       * If context is insufficient, say: â€œI do not have enough information.â€
   * Insert top chunks as context, with clear delimiters and metadata (doc title, section, page range).

4. **LLM generation**

   * Call OpenAI with:

     * System prompt
     * User question
     * Context chunks
   * Model: GPT-4 class with a conservative temperature (e.g., 0.1â€“0.2 for factual tasks).

5. **Post-processing**

   * Check that the answer includes citations like: `[doc: protocol_023, pages 3â€“4]`.
   * If no citations or obviously hallucinated references, either:

     * Retry with a stronger instruction prompt, or
     * Return a refusal: â€œThe context provided does not support a reliable answer.â€

6. **Response**

   * Return:

     * `answer_text`
     * `citations` array:

       * `document_id`
       * `title`
       * `page_range`
       * `doi` or internal link
     * Retrieval metadata (chunks used, similarity scores)
     * Timestamp

---

## ğŸ§± Prompt Guardrails & Traceability

### Guardrails

The system-level instructions include rules like:

* â€œAnswer **only** using the provided documents.â€
* â€œQuote or summarize methods exactly; do not invent missing steps.â€
* â€œEvery answer must include explicit citations mapping to document IDs and pages.â€
* â€œIf the context is insufficient, state that clearly and do not guess.â€

We enforce this by:

* Checking that citations appear in the answer
* Rejecting answers with document names or IDs that were not part of the prompt
* Logging failures for later examination

### Traceability

For every answer, we store:

* `question_id`
* `question_text`
* `selected_chunk_ids`
* `document_ids` used
* `model_name`
* `prompt_version`
* Full prompt text (for reproducibility)
* `answer_text`
* `citations` extracted from answer

This is saved in a table like `rag_answers` and also optionally written to JSONL logs.

---

## ğŸ“Š Evaluation & â€œ60% Review Time Reductionâ€

To approximate the **60% reduction** in review effort, we ran an internal evaluation:

1. **Build an evaluation set**

   * 50 realistic questions from scientists and analysts, such as:

     * â€œWhat temperature and duration did we use for the secondary antibody incubation in protocol 023?â€
     * â€œWhich behavioral metrics were used for the anxiety assay in the 2024 study?â€
     * â€œWhat sample size did we use for the pilot imaging experiment with condition XYZ?â€
   * For each question, we identified a â€œgoldâ€ answer by manual reading.

2. **Manual baseline**

   * 3 reviewers answered the questions by manually searching protocols and papers.
   * We recorded per-question time (minutes) and difficulty.

3. **RAG-assisted condition**

   * Same reviewers, same questions, but allowed to use Helios-Lab.
   * They could:

     * Ask the assistant
     * Inspect citations
     * Click through to the specific pages

4. **Results (hypothetical but realistic)**

   * Median time per question:

     * Manual: ~8 minutes
     * With Helios-Lab: ~3 minutes
   * Relative reduction ~62.5% in average time spent.
   * Accuracy judged by reviewers:

     * 90% of RAG-assisted answers **fully correct**
     * 8% **partially correct** but required minor manual confirmation
     * 2% **insufficient** (tool honesty: â€œnot enough contextâ€)

We report this as **â€œreduced scientific literature and protocol review effort by ~60%.â€**

---

## âš™ï¸ Deployment Pattern

* **Environment**:

  * Docker container with Python + FAISS + OpenAI client
  * Hosted on an internal EC2 instance or container service
* **API**:

  * FastAPI endpoint `/ask`:

    * Input: question text + optional filters (document type, date range)
    * Output: answer, citations, metadata
* **Authentication**:

  * Basic token authentication for internal users
* **Scheduling / Updates**:

  * Nightly job to scan for new documents
  * Rebuild embeddings and FAISS index incrementally if new docs appear

---

## ğŸ§ª Example Usage

### API Example (pseudo)

**Request:**

```json
{
  "question": "What blocking buffer and incubation time did we use in protocol 047 for the Western blot?",
  "filters": {
    "document_type": "protocol"
  }
}
```

**Response (simplified):**

```json
{
  "answer": "In protocol 047, the Western blot used a 5% BSA blocking buffer in TBST for 1 hour at room temperature before primary antibody incubation [doc: protocol_047, pages 4â€“5].",
  "citations": [
    {
      "document_id": "protocol_047",
      "title": "Western Blot Protocol - Pilot Study",
      "page_start": 4,
      "page_end": 5,
      "doi": null,
      "internal_url": "s3://helios/protocols/protocol_047.pdf"
    }
  ],
  "metadata": {
    "chunks_used": ["chunk_047_12", "chunk_047_13"],
    "similarity_scores": [0.87, 0.84],
    "model": "gpt-4-class-model",
    "timestamp": "2025-01-10T14:23:45Z"
  }
}
```

---

## ğŸ” Limitations and Future Improvements

* Does not yet handle **tables and figures** optimally
* Very long methods sections sometimes truncated, we mitigate with careful chunking and retrieval configuration
* Currently no built-in UI for crowdsourced correction; thatâ€™s on the roadmap
* Sensitive data access restricted by IAM and document-level access rules, but a full ABAC/RBAC permissions model is a future enhancement
* For some edge-case questions, the assistant will correctly answer â€œinsufficient contextâ€ rather than inventing an answer â€” this is by design

**Future roadmap:**

* Add a small reviewer interface for **feedback labeling**
* Add **question type classification** (methods vs results vs conceptual)
* Improve indexing for figures and supplementary materials
* Adapt the same RAG framework to:

  * fraud playbooks
  * transaction investigation guides
  * policy and compliance documents

---

